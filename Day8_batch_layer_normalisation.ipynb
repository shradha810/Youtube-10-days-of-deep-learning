{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Implementing Batch and Layer Normalisation in NN using Pytorch\n",
        "\n"
      ],
      "metadata": {
        "id": "VLPTqriIbyiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Input data (batch size=4, number of previous layer neurons=2)\n",
        "before_values = [[4, 2], [3, 1], [4, 5], [6, 2]]\n",
        "\n",
        "# Convert data to PyTorch tensor\n",
        "X = torch.tensor(before_values, dtype=torch.float32)\n",
        "print(f'Original Values: {X}')\n",
        "print(f'X shape: {X.shape}')\n",
        "\n",
        "# Apply batch normalisation\n",
        "def batch_normalisation(X):\n",
        "  # Calculate mean and variance for each neuron's output\n",
        "  mean = torch.mean(X, dim=0, keepdim=True) #Batch dimension\n",
        "  print(f'mean: {mean}')\n",
        "  std = torch.std(X, dim=0, keepdim=True)\n",
        "  print(f'standard deviation: {std}')\n",
        "\n",
        "  # Apply batch normalisation\n",
        "  gamma = torch.ones_like(mean)  # scale parameter\n",
        "  beta = torch.zeros_like(mean)  # drift parameter\n",
        "  epsilon = 1e-8  # small constant to avoid division by zero\n",
        "  normalised_values = (X - mean) / (std + epsilon)\n",
        "  return normalised_values\n",
        "\n",
        "def layer_normalisation(X):\n",
        "  # Calculate mean and variance for each datapoint\n",
        "  mean = torch.mean(X, dim=1, keepdim=True) #Feature Dimension\n",
        "  print(f'mean: {mean}')\n",
        "  std = torch.std(X, dim=1, keepdim=True)\n",
        "  print(f'standard deviation: {std}')\n",
        "\n",
        "  # Apply layer normalisation\n",
        "  gamma = torch.ones_like(mean)  # scale parameter\n",
        "  beta = torch.zeros_like(mean)  # drift parameter\n",
        "  epsilon = 1e-8  # small constant to avoid division by zero\n",
        "  normalised_values = gamma * (X - mean) / (std + epsilon) + beta\n",
        "  return normalised_values\n",
        "\n",
        "print(f'\\n----BATCH NORMALISATION----')\n",
        "batch_normalised_values = batch_normalisation(X)\n",
        "print(f'Output after applying Batch Normalisation: {batch_normalised_values}')\n",
        "\n",
        "print(f'\\n----LAYER NORMALISATION----')\n",
        "layer_normalised_values = layer_normalisation(X)\n",
        "print(f'Output after applying Layer Normalisation: {layer_normalised_values}')"
      ],
      "metadata": {
        "id": "04jnxqP292hZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8bfe9e6-6ae2-4d14-bf6d-aff50e0402ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Values: tensor([[4., 2.],\n",
            "        [3., 1.],\n",
            "        [4., 5.],\n",
            "        [6., 2.]])\n",
            "X shape: torch.Size([4, 2])\n",
            "\n",
            "----BATCH NORMALISATION----\n",
            "mean: tensor([[4.2500, 2.5000]])\n",
            "standard deviation: tensor([[1.2583, 1.7321]])\n",
            "Output after applying Batch Normalisation: tensor([[-0.1987, -0.2887],\n",
            "        [-0.9934, -0.8660],\n",
            "        [-0.1987,  1.4434],\n",
            "        [ 1.3908, -0.2887]])\n",
            "\n",
            "----LAYER NORMALISATION----\n",
            "mean: tensor([[3.0000],\n",
            "        [2.0000],\n",
            "        [4.5000],\n",
            "        [4.0000]])\n",
            "standard deviation: tensor([[1.4142],\n",
            "        [1.4142],\n",
            "        [0.7071],\n",
            "        [2.8284]])\n",
            "Output after applying Layer Normalisation: tensor([[ 0.7071, -0.7071],\n",
            "        [ 0.7071, -0.7071],\n",
            "        [-0.7071,  0.7071],\n",
            "        [ 0.7071, -0.7071]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "    super(Net, self).__init__()\n",
        "    self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "    self.batch_norm1 = nn.BatchNorm1d(hidden_dim)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.layer_norm2 = nn.LayerNorm(hidden_dim)\n",
        "    self.relu2 = nn.ReLU()\n",
        "\n",
        "    self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.fc1(x)\n",
        "    x = self.batch_norm1(x)  # Apply batch normalization\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.fc2(x)\n",
        "    x = self.layer_norm2(x)  # Apply layer normalization\n",
        "    x = self.relu2(x)\n",
        "\n",
        "    x = self.fc3(x)\n",
        "    x = self.sigmoid(x)\n",
        "    return x\n",
        "\n",
        "net = Net(input_dim=8, hidden_dim=16, output_dim=1)\n",
        "print(net)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWQIJ-n9N0vj",
        "outputId": "90dd6d17-052e-4c74-dbf5-78ff1ed7b0dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=8, out_features=16, bias=True)\n",
            "  (batch_norm1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU()\n",
            "  (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
            "  (layer_norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
            "  (relu2): ReLU()\n",
            "  (fc3): Linear(in_features=16, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2pNspJ7OfuTB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}